{"cells":[{"metadata":{"id":"rSvr3eqTltI0"},"cell_type":"markdown","source":"# **Prediction of Stocks Prices with Deep Learning**"},{"metadata":{"id":"qJy0q4m9GoMs","outputId":"338eea9b-93c2-4547-90e5-d9631a654c8e","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom IPython.display import display, Javascript, clear_output\nimport os\nimport shutil\nfrom datetime import datetime\n\n!pip install --upgrade tensorflow==2.4.0-rc4\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\ntf.keras.backend.set_floatx('float64')\n\n!pip install --upgrade scikit-learn==0.24.0rc1\n\n!pip install yahooquery\nfrom yahooquery import Ticker\n\n!rm -rf stocks\n!git clone https://github.com/Talendar/stocks_prices_prediction stocks\n\n%load_ext autoreload\n%autoreload 2\n\nfrom stocks.aux.stocks_data import MultiStocksDataset\nfrom stocks.aux.eval import *\nfrom stocks.aux.normalization import *\nfrom stocks.aux.tf_callbacks import ClearCallback","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting tensorflow==2.4.0-rc4\n  Downloading tensorflow-2.4.0rc4-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n\u001b[K     |████████████████████████████████| 394.7 MB 25 kB/s s eta 0:00:01��████                      | 124.7 MB 49.8 MB/s eta 0:00:06     |█████████████▌                  | 166.4 MB 48.5 MB/s eta 0:00:05        | 171.2 MB 48.5 MB/s eta 0:00:05     |██████████████                  | 172.6 MB 48.5 MB/s eta 0:00:05     |███████████████                 | 184.1 MB 62.6 MB/s eta 0:00:04�█▌ | 376.1 MB 34.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (0.3.3)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (1.1.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (0.2.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (3.3.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (1.1.2)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (0.10.0)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (1.12.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (1.12)\nRequirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (2.10.0)\nRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (2.4.0)\nRequirement already satisfied: typing-extensions~=3.7.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (3.7.4.1)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (1.6.3)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (3.14.0)\nCollecting grpcio~=1.32.0\n  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n\u001b[K     |████████████████████████████████| 3.8 MB 33.9 MB/s eta 0:00:01\n\u001b[?25hCollecting numpy~=1.19.2\n  Downloading numpy-1.19.4-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n\u001b[K     |████████████████████████████████| 14.5 MB 40.4 MB/s eta 0:00:01\n\u001b[?25hCollecting six~=1.15.0\n  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (2.23.0)\nRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (3.14.0)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (1.7.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (3.2.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (1.23.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (1.0.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (46.1.3.post20200325)\nRequirement already satisfied: absl-py~=0.10 in /opt/conda/lib/python3.7/site-packages (from tensorflow==2.4.0-rc4) (0.10.0)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0-rc4) (4.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0-rc4) (3.1.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0-rc4) (0.2.7)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (46.1.3.post20200325)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (1.23.0)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0-rc4) (1.2.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (46.1.3.post20200325)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0-rc4) (0.4.8)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0-rc4) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0-rc4) (1.25.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0-rc4) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0-rc4) (2020.12.5)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow==2.4.0-rc4) (2.23.0)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0-rc4) (3.0.1)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0-rc4) (0.4.8)\nCollecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n  Downloading tensorflow_estimator-2.4.0rc0-py2.py3-none-any.whl (462 kB)\n\u001b[K     |████████████████████████████████| 462 kB 41.1 MB/s eta 0:00:01\n\u001b[?25hCollecting wheel~=0.35\n  Downloading wheel-0.36.1-py2.py3-none-any.whl (34 kB)\nInstalling collected packages: six, wheel, numpy, grpcio, tensorflow-estimator, tensorflow\n  Attempting uninstall: six\n    Found existing installation: six 1.14.0\n    Uninstalling six-1.14.0:\n      Successfully uninstalled six-1.14.0\n  Attempting uninstall: wheel\n    Found existing installation: wheel 0.34.2\n    Uninstalling wheel-0.34.2:\n      Successfully uninstalled wheel-0.34.2\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.18.5\n    Uninstalling numpy-1.18.5:\n      Successfully uninstalled numpy-1.18.5\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.34.0\n    Uninstalling grpcio-1.34.0:\n      Successfully uninstalled grpcio-1.34.0\n  Attempting uninstall: tensorflow-estimator\n    Found existing installation: tensorflow-estimator 2.3.0\n    Uninstalling tensorflow-estimator-2.3.0:\n      Successfully uninstalled tensorflow-estimator-2.3.0\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.3.1\n    Uninstalling tensorflow-2.3.1:\n      Successfully uninstalled tensorflow-2.3.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nucx-py 0.16.0 requires pynvml, which is not installed.\nkubernetes 10.1.0 requires pyyaml~=3.12, but you have pyyaml 5.3.1 which is incompatible.\njupyterlab-git 0.10.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.0.0 which is incompatible.\ngoogle-cloud-pubsub 1.4.3 requires google-api-core[grpc]<1.17.0,>=1.14.0, but you have google-api-core 1.23.0 which is incompatible.\nearthengine-api 0.1.244 requires google-api-python-client>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\ndatashader 0.11.1 requires numba!=0.49.*,!=0.50.*,>=0.37.0, but you have numba 0.49.1 which is incompatible.\nbokeh 2.2.3 requires tornado>=5.1, but you have tornado 5.0.2 which is incompatible.\nastroid 2.3.3 requires wrapt==1.11.*, but you have wrapt 1.12.1 which is incompatible.\naiobotocore 1.1.2 requires botocore<1.17.45,>=1.17.44, but you have botocore 1.19.31 which is incompatible.\u001b[0m\nSuccessfully installed grpcio-1.32.0 numpy-1.19.4 six-1.15.0 tensorflow-2.4.0rc4 tensorflow-estimator-2.4.0rc0 wheel-0.36.1\n","name":"stdout"},{"output_type":"stream","text":"Collecting scikit-learn==0.24.0rc1\n  Downloading scikit_learn-0.24.0rc1-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n\u001b[K     |████████████████████████████████| 22.3 MB 661 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.0rc1) (1.19.4)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.0rc1) (0.14.1)\nRequirement already satisfied: scipy>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.0rc1) (1.4.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.0rc1) (2.1.0)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from scikit-learn==0.24.0rc1) (1.19.4)\nInstalling collected packages: scikit-learn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 0.23.2\n    Uninstalling scikit-learn-0.23.2:\n      Successfully uninstalled scikit-learn-0.23.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nautogluon-core 0.0.15b20201207 requires scikit-learn<0.24,>=0.22.0, but you have scikit-learn 0.24.0rc1 which is incompatible.\u001b[0m\nSuccessfully installed scikit-learn-0.24.0rc1\nCollecting yahooquery\n  Downloading yahooquery-2.2.8.tar.gz (45 kB)\n\u001b[K     |████████████████████████████████| 45 kB 146 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: lxml==4.5.0 in /opt/conda/lib/python3.7/site-packages (from yahooquery) (4.5.0)\nRequirement already satisfied: pandas>=0.24 in /opt/conda/lib/python3.7/site-packages (from yahooquery) (1.1.5)\nRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24->yahooquery) (1.19.4)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24->yahooquery) (2019.3)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=0.24->yahooquery) (2.8.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.24->yahooquery) (1.15.0)\nCollecting requests-futures==1.0.0\n  Downloading requests-futures-1.0.0.tar.gz (10 kB)\nRequirement already satisfied: requests>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from requests-futures==1.0.0->yahooquery) (2.23.0)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (1.25.9)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (2020.12.5)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (3.0.4)\nCollecting selenium==3.141.0\n  Downloading selenium-3.141.0-py2.py3-none-any.whl (904 kB)\n\u001b[K     |████████████████████████████████| 904 kB 2.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=1.2.0->requests-futures==1.0.0->yahooquery) (1.25.9)\nBuilding wheels for collected packages: yahooquery, requests-futures\n  Building wheel for yahooquery (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for yahooquery: filename=yahooquery-2.2.8-py3-none-any.whl size=48255 sha256=cce0afffccdcf72f81c7d1fee81718559e6ea50f46179e4b4e5aa8b90671ed2f\n  Stored in directory: /root/.cache/pip/wheels/c7/62/b9/d21f1fe2761add492f47217b379d4e927290962c95a087a7ee\n  Building wheel for requests-futures (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for requests-futures: filename=requests_futures-1.0.0-py3-none-any.whl size=7014 sha256=f9e5de2fae544fbc116daa7cf2ff6aa6fc3bd044234976c911be86bbf63cfab4\n  Stored in directory: /root/.cache/pip/wheels/e6/5d/70/8aa32975ceceb612d545b74897e6e8dcc399d78522458c3004\nSuccessfully built yahooquery requests-futures\nInstalling collected packages: selenium, requests-futures, yahooquery\nSuccessfully installed requests-futures-1.0.0 selenium-3.141.0 yahooquery-2.2.8\nCloning into 'stocks'...\nremote: Enumerating objects: 223, done.\u001b[K\nremote: Counting objects: 100% (223/223), done.\u001b[K\nremote: Compressing objects: 100% (223/223), done.\u001b[K\nremote: Total 223 (delta 137), reused 0 (delta 0), pack-reused 0\u001b[K\nReceiving objects: 100% (223/223), 8.50 MiB | 12.36 MiB/s, done.\nResolving deltas: 100% (137/137), done.\n","name":"stdout"}]},{"metadata":{"id":"ZMK0K7CieCES"},"cell_type":"markdown","source":"## **0) Base settings**"},{"metadata":{"id":"X0M3_vwgmls8","trusted":true},"cell_type":"code","source":"NUM_SESSIONS = 10  # number of previous trading sessions the model will analyse in order to make a prediction\nNORMALIZE_LABELS = True\n\nTRAIN_PC, VAL_PC, TEST_PC = 0.75, 0.15, 0.1\nassert (TRAIN_PC + VAL_PC + TEST_PC) == 1\n\nLABELS_NAMES = [    # values that the model will try to predict\n    \"open\", \"low\", \"high\",\n]\n\nINTERVAL = \"1d\"\nNAME_LIST = set([\n    # South America\n    \"^BVSP\",                                        # Brazil\n    # North America\n    \"^DJI\", \"^GSPC\", \"^IXIC\", \"^NYA\", \"^RUT\",       # US\n    \"^MXX\",                                         # Mexico\n    \"^GSPTSE\",                                      # Canada\n    # Europe\n    \"^FTSE\", \"^FCHI\", \"^GDAXI\", \"^IBEX\", \"^AEX\", \"^ATX\",\n    \"^N100\",  \"^BFX\", \"^OMX\",\n    # Asia\n    \"000001.SS\", \"^HSI\", \"399001.SZ\", \"^TWII\",      # China\n    \"^N225\",                                        # Japan\n    \"^KS11\",                                        # S. Korea \n    \"^BSESN\", \"^NSEI\",                              # India\n    \"TA35.TA\",                                      # Israel\n    # Oceania\n    \"^AORD\",                                        # Australia\n    \n])\n\n# loading stocks with specific params:\nSTOCKS = {\n}\n\n# loading stocks from NAME_LIST with default info (same params for all):\nSTOCKS.update({name: {\"start\": \"2007-01-01\",\n                      \"end\": \"2020-12-08\",\n                      \"period\": None} \\\n               for name in NAME_LIST})","execution_count":5,"outputs":[]},{"metadata":{"id":"6L4BVEhAlIdI"},"cell_type":"markdown","source":"## **1) Preparing the data**"},{"metadata":{"id":"iZ_jINp0jq6M"},"cell_type":"markdown","source":"#### **1.1) Fetching and pre-processing the data**"},{"metadata":{"id":"e3xfb2F33Ruy","trusted":true},"cell_type":"code","source":"MAX_ZERO_VOL_PC = 0.5\nmulti_data = {}\n\nfor name, info in STOCKS.items():\n    hist = Ticker(name).history(\n        start=info[\"start\"],\n        end=info[\"end\"],\n        period=info[\"period\"], \n        interval=INTERVAL\n    ).reset_index(\"symbol\", drop=True)\n    hist = hist[[\"high\", \"close\", \"open\", \"low\", \"volume\"]]\n    \n    zero_vols_pc = hist[\"volume\"].isin([0]).sum() / len(hist)\n    if zero_vols_pc > MAX_ZERO_VOL_PC:\n        print(f\"[WARNING] {100*zero_vols_pc : .2f}% of the\" + \\\n              f\"volumes of {name} are 0! Skipping symbol...\")\n    else:\n        multi_data[name] = hist\n\n\nmulti_data = MultiStocksDataset(\n    stocks=multi_data, \n    num_sessions=NUM_SESSIONS,\n    labels_names=LABELS_NAMES,\n    batch_size=len(STOCKS) * 5,\n    data_split_pc=(TRAIN_PC, VAL_PC, TEST_PC),\n    feature_normalization=(min_max_norm, min_max_denorm), \n    label_normalization=(min_max_norm, min_max_denorm) \\\n                        if NORMALIZE_LABELS else None,\n)","execution_count":6,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"from_generator() got an unexpected keyword argument 'output_signature'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-480e47112037>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mfeature_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_max_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_max_denorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlabel_normalization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_max_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_max_denorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mNORMALIZE_LABELS\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m )\n","\u001b[0;32m/kaggle/working/stocks/aux/stocks_data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, stocks, num_sessions, labels_names, batch_size, data_split_pc, feature_normalization, label_normalization)\u001b[0m\n\u001b[1;32m    163\u001b[0m         self._tf_datasets = {\n\u001b[1;32m    164\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_MultiDatasetWrapperTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_ds\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         }\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/stocks/aux/stocks_data.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    163\u001b[0m         self._tf_datasets = {\n\u001b[1;32m    164\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_MultiDatasetWrapperTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_ds\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"val\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         }\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/stocks/aux/stocks_data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, mode)\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             self._tf_ds = tf.data.Dataset.from_generator(\n\u001b[0;32m--> 197\u001b[0;31m                 self._generator, output_signature=root._out_sign)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_make_iters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: from_generator() got an unexpected keyword argument 'output_signature'"]}]},{"metadata":{"id":"rOT4J4NOj6mP"},"cell_type":"markdown","source":"#### **1.2) Inspecting the data**"},{"metadata":{"id":"dnMi_O38kDPB","outputId":"f8f8a90a-1386-4183-ab08-2585800729e5","trusted":false},"cell_type":"code","source":"visualize = \"all\"  # \"all\" or list with specific symbols\ntotal_sessions = 0\nfor symbol, sdata in multi_data.stocks:\n    total_sessions += len(sdata.raw)\n    if visualize == \"all\" or symbol in visualize:\n        zero_vols_pc = sdata.raw[\"volume\"].isin([0]).sum() / len(sdata.raw)\n        print(\"\\n\\n\" + \"#\"*35 + f\"   {symbol}   \" + \"#\"*35 + \"\\n\\n\" +\n              f\". Period: from {sdata.raw.index[0]} to {sdata.raw.index[-1]}\\n\" +\n              f\". Trading sessions: {len(sdata.raw)}\\n\" +\n              f\". Zero volumes: {100*zero_vols_pc : .2f}%\\n\" \n              f\". Data:\\n\")\n        display(sdata.raw)\n\n        print(f\"\\n. Statistics:\\n\")\n        display(sdata.raw.describe())\n\n        print(f\"\\n. Plot:\\n\")\n        ax = sdata.raw[\"open\"].plot(figsize=(12,5), color=np.random.rand(1, 3))\n        ax.set_title(f\"{symbol} opening prices\\n\", fontsize=16, color=\"#ffffff\");\n        ax.set_ylabel(\"Opening prices\", fontsize=\"14\", color=\"#ffffff\");\n        ax.set_xlabel(\"Date\", fontsize=\"14\", color=\"#ffffff\");\n        plt.show()\n        print(\"\\n\\n\" + \"#\"*80 + \"\\n\\n\")\n\nprint(f\"Total number of trading sessions: {total_sessions}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"ji9tD48QQA__","outputId":"b06579a2-61a5-4f68-9dce-5cd6382142ca","scrolled":true,"trusted":false},"cell_type":"code","source":"# data division dates\nfor symbol, data in multi_data.stocks:\n    print(f\">>>>> {symbol}\")\n    print(\". Training range: %s to %s\" % (\n        data.raw_train[\"features\"].index[0], data.raw_train[\"features\"].index[-1]))\n    print(\". Validation range: %s to %s\" % (\n        data.raw_val[\"features\"].index[0], data.raw_val[\"features\"].index[-1]))\n    print(\". Test range: %s to %s\\n\" % (\n        data.raw_test[\"features\"].index[0], data.raw_test[\"features\"].index[-1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"eIUPMkiWOJoV","outputId":"8e913861-2c8f-4557-d1c7-b0cac361c934","trusted":false},"cell_type":"code","source":"# comparing normalized values of the training sets\nprint(\"#\"*20 + \" Normalized training data \" + \"#\"*20)\nfor symbol, data in multi_data.stocks:\n    print(f\"\\n>>>>> {symbol}\")\n    display(data.norm_train[\"features\"].tail())","execution_count":null,"outputs":[]},{"metadata":{"id":"M8AHaxiLmPX3","outputId":"bdf5afb8-5268-43b6-eb66-106ebcd3fd52","trusted":false},"cell_type":"code","source":"# comparing normalized values of the validation sets\nprint(\"#\"*20 + \" Normalized validation data \" + \"#\"*20)\nfor symbol, data in multi_data.stocks:\n    print(f\"\\n>>>>> {symbol}\")\n    display(data.norm_val[\"features\"].tail())","execution_count":null,"outputs":[]},{"metadata":{"id":"Tpm4JYrYmiWg","outputId":"d63151ca-6687-4fe0-f62e-3cc4db62270b","trusted":false},"cell_type":"code","source":"# comparing normalized values of the test sets\nprint(\"#\"*20 + \" Normalized test data \" + \"#\"*20)\nfor symbol, data in multi_data.stocks:\n    print(f\"\\n>>>>> {symbol}\")\n    display(data.norm_test[\"features\"].tail())","execution_count":null,"outputs":[]},{"metadata":{"id":"1CgfNvpcwj1L","outputId":"dd54d9de-f7a3-4cc2-aa59-b3f606936e9d","trusted":false},"cell_type":"code","source":"# sample output shape\nsample_x, sample_y = next(iter(multi_data.tf_datasets[\"train\"]))\nprint(f\"Sample input shape: {sample_x.shape}\")\nprint(f\"Sample label shape: {sample_y.shape}\")","execution_count":null,"outputs":[]},{"metadata":{"id":"yzFhcWwiUtIi"},"cell_type":"markdown","source":"## **2) Defining a model**"},{"metadata":{"id":"cKQGt8eXUsxu","trusted":false},"cell_type":"code","source":"class MultiLSTM(tf.keras.Model):\n    \"\"\" Custom LSTM model. \"\"\"\n\n    def __init__(self, num_sessions=NUM_SESSIONS, load_path=None):\n        super(MultiLSTM, self).__init__()\n        if load_path is not None:\n            self.custom_load(load_path)\n        else:\n            self._num_sessions = num_sessions\n            self._open_predictor = model = tf.keras.models.Sequential([\n                tf.keras.layers.LSTM(32, return_sequences=True),\n                tf.keras.layers.Dropout(0.25),\n                tf.keras.layers.LSTM(32, return_sequences=True),\n                tf.keras.layers.Dropout(0.25),\n                tf.keras.layers.LSTM(32, return_sequences=False),\n                tf.keras.layers.BatchNormalization(),\n                #tf.keras.layers.Dense(128, activation=\"relu\"),  \n                #tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dense(1, activation=\"relu\"),\n            ])\n\n            self._pre_lowest = tf.keras.models.Sequential([\n                tf.keras.layers.LSTM(32, return_sequences=True)\n            ])\n            self._lowest_predictor = tf.keras.models.Sequential([\n                tf.keras.layers.LSTM(32, return_sequences=True),\n                tf.keras.layers.Dropout(0.25),\n                tf.keras.layers.LSTM(32, return_sequences=True),\n                tf.keras.layers.Dropout(0.25),\n                tf.keras.layers.LSTM(32, return_sequences=False),\n                tf.keras.layers.BatchNormalization(),\n                #tf.keras.layers.Dense(128, activation=\"relu\"),  \n                #tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dense(1, activation=\"relu\"),\n            ])\n\n            self._pre_highest = tf.keras.models.Sequential([\n                tf.keras.layers.LSTM(32, return_sequences=True)\n            ])\n            self._highest_predictor = tf.keras.models.Sequential([\n                tf.keras.layers.LSTM(32, return_sequences=True),\n                tf.keras.layers.Dropout(0.25),\n                tf.keras.layers.LSTM(32, return_sequences=True),\n                tf.keras.layers.Dropout(0.25),\n                tf.keras.layers.LSTM(32, return_sequences=False),\n                tf.keras.layers.BatchNormalization(),\n                #tf.keras.layers.Dense(128, activation=\"relu\"),  \n                #tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dense(1, activation=\"relu\"),\n            ])\n\n    def call(self, inputs, training=None):\n        # calculating the opening price\n        open_price = self._open_predictor(inputs, training=training)\n\n        # cloning prices: expanding shape from (batches, 1) to (batches, NUM_SESSIONS, 1)\n        open_price_exp = tf.tile(tf.expand_dims(open_price, -1),   \n                                 [1, self._num_sessions, 1])\n\n        # calculating the lowest price\n        lowest_price = self._lowest_predictor(\n            # appends the opening price to the end of each item in the input sequence\n            tf.concat([self._pre_lowest(inputs), open_price_exp], -1),\n            training=training,\n        )\n\n        # cloning prices: expanding shape from (batches, 1) to (batches, NUM_SESSIONS, 1)\n        lowest_price_exp = tf.tile(tf.expand_dims(lowest_price, -1),   \n                                   [1, self._num_sessions, 1])\n\n        # calculating the highest price\n        highest_price = self._highest_predictor(\n            # appends the opening price and the lowest price to the end of each item in the input sequence\n            tf.concat(\n                [self._pre_highest(inputs), open_price_exp, lowest_price_exp], -1),\n            training=training,\n        )\n\n        # returning the concatenation of the opening price, lowest price and highest price\n        return tf.concat([open_price, lowest_price, highest_price], -1)\n    \n    def custom_save(self, dir_path):\n        if not os.path.isdir(dir_path):\n            os.makedirs(dir_path)\n            \n        with open(os.path.join(dir_path, \"info.txt\"), \"w\") as file:\n            file.write(\"num_sessions %d\" % self._num_sessions)\n        \n        pred_dir = os.path.join(dir_path, \"predictors\")\n        if not os.path.isdir(pred_dir):\n            os.makedirs(pred_dir)\n            \n        pre_dir = os.path.join(dir_path, \"pre_layers\")\n        if not os.path.isdir(pre_dir):\n            os.makedirs(pre_dir)\n            \n        # saving predictors\n        self._open_predictor.save(os.path.join(pred_dir, \"open_predictor\"))\n        self._open_predictor.save(os.path.join(pred_dir, \"open_predictor.h5\"))\n        \n        self._lowest_predictor.save(os.path.join(pred_dir, \"low_predictor\"))\n        self._lowest_predictor.save(os.path.join(pred_dir, \"low_predictor.h5\"))\n        \n        self._highest_predictor.save(os.path.join(pred_dir, \"high_predictor\"))\n        self._highest_predictor.save(os.path.join(pred_dir, \"high_predictor.h5\"))\n        \n        # saving pre-layers\n        self._pre_lowest.save(os.path.join(pred_dir, \"pre_low\"))\n        self._pre_lowest.save(os.path.join(pred_dir, \"pre_low.h5\"))\n        \n        self._pre_highest.save(os.path.join(pred_dir, \"pre_high\"))\n        self._pre_highest.save(os.path.join(pred_dir, \"pre_high.h5\"))\n    \n    def custom_load(self, dir_path):\n        with open(os.path.join(dir_path, \"info.txt\"), \"r\") as file:\n            self._num_sessions = int(file.readline().split()[1])\n            \n        pred_dir = os.path.join(dir_path, \"predictors\")\n        pre_dir = os.path.join(dir_path, \"pre_layers\")\n        \n        # loading predictors\n        self._open_predictor = tf.keras.models.load_model(os.path.join(pred_dir, \"open_predictor\"))\n        self._lowest_predictor = tf.keras.models.load_model(os.path.join(pred_dir, \"low_predictor\"))\n        self._highest_predictor = tf.keras.models.load_model(os.path.join(pred_dir, \"high_predictor\"))\n        \n        # saving pre-layers\n        self._pre_lowest = tf.keras.models.load_model(os.path.join(pred_dir, \"pre_low\"))\n        self._pre_highest = tf.keras.models.load_model(os.path.join(pred_dir, \"pre_high\"))","execution_count":null,"outputs":[]},{"metadata":{"id":"Bcuz5tM_R9G5","outputId":"763028fb-ad3f-45c8-cd09-152682daa965","trusted":false},"cell_type":"code","source":"# testing output shape\nx, y = next(iter(multi_data.tf_datasets[\"train\"]))\nMultiLSTM()(x).shape","execution_count":null,"outputs":[]},{"metadata":{"id":"dRyvKOCOHPq3"},"cell_type":"markdown","source":"## **3) Training the model**"},{"metadata":{"id":"AaqO1SUVSNSe","outputId":"eb188e17-36f3-4083-e872-c8630ff42986","trusted":false},"cell_type":"code","source":"# loading all validation data into memory (so we dont have to use a generator)\nval_inputs, val_outputs = [], []\nfor x, y in multi_data.tf_datasets[\"val\"]:\n    val_inputs.append(x)\n    val_outputs.append(y)\n\nval_inputs = tf.concat(val_inputs, axis=0)\nval_outputs = tf.concat(val_outputs, axis=0)\n\nprint(val_inputs.shape, val_outputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"o7IKyg0MD4CG","outputId":"4765028b-e542-4cc7-9b7c-08cecee3fccd","trusted":false},"cell_type":"code","source":"save_path = f\"saved_model_{datetime.today().strftime('%Y-%m-%d-%H-%M-%S')}\"\ntf.keras.backend.clear_session()\n\n# building and compiling\nmodel = MultiLSTM()\nmodel.compile(loss=tf.losses.MeanSquaredError(),\n              optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n              metrics=[])\n                       #tf.metrics.MeanAbsoluteError()]) \n                       #tf.metrics.MeanAbsolutePercentageError()])\n\n# training\nepochs = 20\nhistory = model.fit(\n    multi_data.tf_datasets[\"train\"].shuffle(buffer_size=multi_data.size[\"train\"]), \n    epochs=epochs,\n    validation_data=(val_inputs, val_outputs),\n    callbacks=[ClearCallback(),\n               ModelCheckpoint(filepath=os.path.join(save_path, \"checkpoint_best\"), \n                                                     monitor='val_loss', \n                                                     verbose=1, save_best_only=True)],\n)\n\n# restoring the checkpoint of the best model\nmodel.load_weights(os.path.join(save_path, \"checkpoint_best\"))\n\n# saving and downloading the model\nmodel.save(os.path.join(save_path, \"full_save\"))\nmodel.custom_save(save_path)\nshutil.make_archive(save_path, \"zip\", save_path)\n\n# visualizing loss history\nprint(\"\\n\\n\")\nplt.rc('xtick',labelsize=12, color=\"#DCDCDC\")\nplt.rc('ytick',labelsize=12, color=\"#DCDCDC\")\nplt.rcParams.update({'legend.fontsize': 14, 'legend.handlelength': 2})\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(epochs), history.history[\"loss\"], 'r--')\nplt.plot(range(epochs), history.history[\"val_loss\"], 'b-')\nplt.legend(['Training Loss', 'Validation Loss'])\n\nplt.title(\"Loss History\\n\", fontsize=17, color=\"#E0E0E0\")\nplt.ylabel('MSE\\n', fontsize=14, color=\"#E0E0E0\")\nplt.xlabel('\\nEpoch', fontsize=14, color=\"#E0E0E0\")\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#model = MultiLSTM(load_path=\"saved_model_2020-12-10-01-42-11\")\n#model.compile(loss=tf.losses.MeanSquaredError(),\n              #optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n              #metrics=[])","execution_count":null,"outputs":[]},{"metadata":{"id":"RjN-t1jpYiid"},"cell_type":"markdown","source":"## **4) Evaluating on all the stocks**"},{"metadata":{"id":"FWp2Rtt06sUd","outputId":"336ba009-be9a-4de6-f396-b49af6d28806","trusted":false},"cell_type":"code","source":"loss = {}\nfor mode in [\"train\", \"val\", \"test\"]:\n    loss[mode] = model.evaluate(multi_data.tf_datasets[mode])\n\nloss = pd.Series(loss.values(), index=loss.keys())\nprint(\"\\n\\n>>> Loss value (on the normalized data):\")\nloss","execution_count":null,"outputs":[]},{"metadata":{"id":"FcpmQirVkJS3"},"cell_type":"markdown","source":"## **5) Evaluating on individual stocks**"},{"metadata":{"id":"WQvIUlSZY8JH","trusted":false},"cell_type":"code","source":"results, predictions = {}, {}\nfor i, stk in enumerate(STOCKS.keys()):\n    clear_output(wait=True)\n    print(f\"[{100*(i+1) / len(STOCKS) : 2f}%] Evaluating stock {i+1} of {len(STOCKS)}... \", end=\"\")\n\n    results[stk], predictions[stk] = {}, {}\n    for mode in [\"train\", \"val\", \"test\"]:\n        predictions[stk][mode], results[stk][mode] = eval(model, \n                                                          multi_data[stk], mode)\n    print(\"done!\")","execution_count":null,"outputs":[]},{"metadata":{"id":"RuPubrGQkPO2"},"cell_type":"markdown","source":"#### **5.1) Training data**"},{"metadata":{"id":"OfGDZxA-8glW","trusted":false},"cell_type":"code","source":"symbol = \"^BVSP\"","execution_count":null,"outputs":[]},{"metadata":{"id":"yhkAiCHKQBVA","outputId":"7cdf7359-ccd9-46aa-b28d-4694fbb89ab5","trusted":false},"cell_type":"code","source":"print(\"     Training Data\\n\")\neval_print(results[symbol][\"train\"])\neval_plot(predictions[symbol][\"train\"], \n          multi_data[symbol].raw_train[\"labels\"], \n          start_date=\"random\", \n          plot_samples=100, \n          title=\"Performance on the Training Set\")","execution_count":null,"outputs":[]},{"metadata":{"id":"pQHpzVnzkTTC"},"cell_type":"markdown","source":"#### **5.2) Validation data**"},{"metadata":{"id":"HZOXlbdhRo5i","outputId":"3f2ad2be-97d5-493e-858a-b01031bb2462","trusted":false},"cell_type":"code","source":"print(\"     Validation Data\\n\")\neval_print(results[symbol][\"val\"])\neval_plot(predictions[symbol][\"val\"], \n          multi_data[symbol].raw_val[\"labels\"], \n          start_date=\"random\", \n          plot_samples=100, \n          title=\"Performance on the Validation Set\")","execution_count":null,"outputs":[]},{"metadata":{"id":"NgCxssMpkXg0"},"cell_type":"markdown","source":"#### **5.3) Test data**"},{"metadata":{"id":"1PZroVc4kiyZ","outputId":"4666c58a-b54d-446d-cc44-e3bcdb292bdb","trusted":false},"cell_type":"code","source":"print(\"    Test Data\\n\")\neval_print(results[symbol][\"test\"])\neval_plot(predictions[symbol][\"test\"], \n          multi_data[symbol].raw_test[\"labels\"], \n          start_date=\"random\", \n          plot_samples=100, \n          title=\"Performance on the Test Set\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}